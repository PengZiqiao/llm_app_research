{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ğŸ‘‹! I'm ChatGLMï¼ˆæ™ºè°±æ¸…è¨€ï¼‰, the artificial intelligence assistant, nice to meet you. Feel free to ask me any questions.\n"
     ]
    }
   ],
   "source": [
    "from model import client\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='glm-4-0520',\n",
    "    messages=[{\"role\": \"user\", \"content\": \"hello glm!\"}],\n",
    "    temperature=0.2,\n",
    "    max_tokens=4096,\n",
    "    stream=False\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.tools.python.tool import PythonAstREPLTool\n",
    "import pandas as pd\n",
    "\n",
    "# python_repl_ast\n",
    "df = pd.read_excel(\"titanic.xlsx\")\n",
    "python_repl_ast = PythonAstREPLTool(locals={'df':df})\n",
    "\n",
    "# å·¥å…·æè¿°ä¸å­—å…¸\n",
    "tools_description = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"python_repl_ast\",\n",
    "        \"description\": \"ä¸€ä¸ªPython shellã€‚ä½¿ç”¨è¿™ä¸ªå·¥å…·æ¥æ‰§è¡ŒPythonå‘½ä»¤ã€‚è¾“å…¥åº”è¯¥æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„Pythonå‘½ä»¤ã€‚åœ¨ä½¿ç”¨è¿™ä¸ªå·¥å…·æ—¶ï¼Œæœ‰æ—¶å€™è¾“å‡ºä¼šè¢«çœç•¥â€”â€”åœ¨ä½¿ç”¨å®ƒå›ç­”ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®ƒçœ‹èµ·æ¥æ²¡æœ‰è¢«çœç•¥ã€‚\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"tool_input\": {\n",
    "                    \"description\": \"ä¸€ä¸ªæœ‰æ•ˆçš„Pythonå‘½ä»¤\",\n",
    "                    \"type\": \"str\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\n",
    "                \"tool_input\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "tools_map = {\n",
    "    \"python_repl_ast\": python_repl_ast\n",
    "}\n",
    "\n",
    "tools = {\n",
    "    \"description\": tools_description,\n",
    "    \"map\": tools_map\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import tools_description, tools_map\n",
    "import json\n",
    "\n",
    "def predict(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model='glm-4-0520',\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "        tools = tools_description,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '', 'role': 'assistant', 'tool_calls': [{'id': 'call_2024082911361346d843553d984253', 'function': {'arguments': '{\"tool_input\": \"primes = []\\\\nfor num in range(2, 51):\\\\n    is_prime = True\\\\n    for i in range(2, int(num ** 0.5) + 1):\\\\n        if num % i == 0:\\\\n            is_prime = False\\\\n            break\\\\n    if is_prime:\\\\n        primes.append(num)\\\\nprimes\"}', 'name': 'python_repl_ast'}, 'type': 'function', 'index': 0}]}\n"
     ]
    }
   ],
   "source": [
    "prompt = 'ä½¿ç”¨pythonæ‰¾å‡º50ä»¥å†…çš„è´¨æ•°'\n",
    "messages = [{'role':'user', 'content':prompt}]\n",
    "\n",
    "response_messages = predict(messages)\n",
    "print(response_messages)\n",
    "\n",
    "messages.append(response_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'call_2024082911361346d843553d984253',\n",
       "  'function': {'arguments': '{\"tool_input\": \"primes = []\\\\nfor num in range(2, 51):\\\\n    is_prime = True\\\\n    for i in range(2, int(num ** 0.5) + 1):\\\\n        if num % i == 0:\\\\n            is_prime = False\\\\n            break\\\\n    if is_prime:\\\\n        primes.append(num)\\\\nprimes\"}',\n",
       "   'name': 'python_repl_ast'},\n",
       "  'type': 'function',\n",
       "  'index': 0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_calls_obj = response_messages['tool_calls']\n",
    "tools_calls_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_call(tools_calls_obj):\n",
    "    function_obj = tools_calls_obj[0]['function']\n",
    "\n",
    "    func = tools_map[function_obj['name']]\n",
    "    args = json.loads(function_obj['arguments'])\n",
    "\n",
    "    return json.dumps(func(**args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17467/1909064332.py:7: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use invoke instead.\n",
      "  return json.dumps(func(**args))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_call(tools_calls_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'ä½¿ç”¨pythonæ‰¾å‡º50ä»¥å†…çš„è´¨æ•°'},\n",
       " {'content': '',\n",
       "  'role': 'assistant',\n",
       "  'tool_calls': [{'id': 'call_2024082911361346d843553d984253',\n",
       "    'function': {'arguments': '{\"tool_input\": \"primes = []\\\\nfor num in range(2, 51):\\\\n    is_prime = True\\\\n    for i in range(2, int(num ** 0.5) + 1):\\\\n        if num % i == 0:\\\\n            is_prime = False\\\\n            break\\\\n    if is_prime:\\\\n        primes.append(num)\\\\nprimes\"}',\n",
       "     'name': 'python_repl_ast'},\n",
       "    'type': 'function',\n",
       "    'index': 0}]},\n",
       " {'role': 'tool',\n",
       "  'content': '[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]',\n",
       "  'tool_call_id': 'call_2024082911361346d843553d984253'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.append({\n",
    "    \"role\":\"tool\",\n",
    "    \"content\": func_call(tools_calls_obj),\n",
    "    \"tool_call_id\": tools_calls_obj[0][\"id\"]\n",
    "})\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '50ä»¥å†…çš„è´¨æ•°æœ‰ï¼š2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47ã€‚',\n",
       " 'role': 'assistant',\n",
       " 'tool_calls': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from string import Template\n",
    "from model import client\n",
    "import json\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, tools, prompt_template_name=None):\n",
    "        self.messages = []\n",
    "        self.tools_description = tools['description']\n",
    "        self.tools_map = tools['map']\n",
    "\n",
    "        if prompt_template_name:\n",
    "            self.template = Template(Path(f'templates/{prompt_template_name}.txt').read_text())\n",
    "        else:\n",
    "            self.template = None\n",
    "\n",
    "\n",
    "    def prompt_template_format(self, **kwargs):\n",
    "        return self.template.substitute(**kwargs)\n",
    "    \n",
    "    def predict(self, messages):\n",
    "        response = client.chat.completions.create(\n",
    "            model='glm-4-0520',\n",
    "            messages=messages,\n",
    "            temperature=0,\n",
    "            tools = self.tools_description,\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.model_dump()\n",
    "\n",
    "    def func_call(self, tools_calls_obj):\n",
    "        function_obj = tools_calls_obj[0]['function']\n",
    "\n",
    "        func = self.tools_map[function_obj['name']]\n",
    "        args = json.loads(function_obj['arguments'])\n",
    "\n",
    "        return json.dumps(func(**args))\n",
    "    \n",
    "    def __call__(self, input):\n",
    "\n",
    "        # å¦‚æœæ¨¡æ¿å­˜åœ¨ï¼Œæ ¹æ®æ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥\n",
    "        if self.template:\n",
    "            input = self.prompt_template_format(input=input)\n",
    "\n",
    "        # ç¬¬ä¸€æ¬¡è°ƒç”¨å¤§æ¨¡å‹\n",
    "        self.messages.append({'role': 'user', 'content': input})\n",
    "        response_messages = self.predict(self.messages)\n",
    "        self.messages.append(response_messages)\n",
    "\n",
    "        # å½“éœ€è¦è°ƒç”¨å‡½æ•°æ—¶ï¼Œæ‰§è¡Œfunc_callï¼Œå¹¶å°†è¿”å›å€¼æ·»åŠ åˆ°messageä¸­ï¼Œå†ä¸€æ¬¡è°ƒç”¨å¤§æ¨¡å‹\n",
    "        while ('tool_calls' in response_messages) and (response_messages['tool_calls'] is not None):\n",
    "            tools_calls_obj = response_messages['tool_calls']\n",
    "            self.messages.append({\n",
    "                \"role\":\"tool\",\n",
    "                \"content\": self.func_call(tools_calls_obj),\n",
    "                \"tool_call_id\": tools_calls_obj[0][\"id\"]\n",
    "            })\n",
    "            response_messages = self.predict(self.messages)\n",
    "\n",
    "        return response_messages['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import tools\n",
    "\n",
    "agent = Agent(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å°æ˜ä»Šå¹´4å²ï¼Œèº«é«˜100cmã€‚å¦‚æœæ¯å¹´çš„èº«é«˜å¢åŠ 10cmï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹è®¡ç®—è¿‡ç¨‹æ¥å¾—å‡ºå°æ˜80å²æ—¶çš„èº«é«˜ï¼š\n",
      "\n",
      "1. å°æ˜å½“å‰çš„å¹´é¾„æ˜¯4å²ã€‚\n",
      "2. å°æ˜å½“å‰çš„èº«é«˜æ˜¯100cmã€‚\n",
      "3. æ¯å¹´èº«é«˜å¢åŠ 10cmã€‚\n",
      "4. å°æ˜åˆ°80å²æ—¶ï¼Œè¿˜éœ€è¦è¿‡80 - 4 = 76å¹´ã€‚\n",
      "5. åœ¨è¿™76å¹´é‡Œï¼Œå°æ˜çš„èº«é«˜å°†å¢åŠ 76 * 10cm = 760cmã€‚\n",
      "6. å› æ­¤ï¼Œå°æ˜80å²æ—¶çš„èº«é«˜å°†æ˜¯åˆå§‹èº«é«˜åŠ ä¸Šè¿™76å¹´çš„å¢é•¿ï¼Œå³100cm + 760cm = 860cmã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œå°æ˜80å²æ—¶çš„èº«é«˜æ˜¯860cmã€‚\n"
     ]
    }
   ],
   "source": [
    "print(agent('ä½¿ç”¨pythonè®¾è®¡ç¨‹åºï¼Œå›ç­”â€œå°æ˜ä»Šå¹´4å²ï¼Œèº«é«˜100cmã€‚å¦‚æœæ¯å¹´çš„èº«é«˜å¢åŠ 10cmï¼Œé‚£ä¹ˆå°æ˜80å²æ—¶çš„èº«é«˜æ˜¯å¤šå°‘ï¼Ÿâ€ï¼Œå¹¶è§£é‡Šè®¡ç®—è¿‡ç¨‹'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
